{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import edward as ed\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy.random as npr\n",
    "\n",
    "from tensorflow.contrib import slim\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from edward.models import Uniform\n",
    "from edward.models import Categorical, InverseGamma, Mixture, MultivariateNormalDiag, Normal\n",
    "from scipy.stats import multivariate_normal as mnormal\n",
    "import tensorflow.contrib.distributions as tfd\n",
    "\n",
    "ed.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "M = 64 # batch size during training\n",
    "leak = 0.2 # leak parameter for leakyrelu\n",
    "num_iter = 40000\n",
    "DIR = \"../../../data/bags2shoes\"\n",
    "\n",
    "IMG_DIR = \"../plots/bags2shoes_shared_rev_4\"\n",
    "\n",
    "if os.path.exists(IMG_DIR):\n",
    "    shutil.rmtree(IMG_DIR)\n",
    "\n",
    "os.makedirs(IMG_DIR)\n",
    "os.makedirs(IMG_DIR+'/trainA/')\n",
    "os.makedirs(IMG_DIR+'/trainB/')\n",
    "os.makedirs(IMG_DIR+'/trainBA/')\n",
    "os.makedirs(IMG_DIR+'/trainAB/')\n",
    "os.makedirs(IMG_DIR+'/trainABA/')\n",
    "os.makedirs(IMG_DIR+'/trainBAB/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "from PIL import Image\n",
    "if len(glob.glob(DIR + '/*.npy')) == 0:\n",
    "    filelist_1 = glob.glob(DIR + '/bags/*.jpg')\n",
    "    filelist_2 = glob.glob(DIR + '/shoes/*.jpg')\n",
    "    xs = np.array([np.array(Image.open(fname)) for fname in filelist_1])\n",
    "    ys = np.array([np.array(Image.open(fname)) for fname in filelist_2])\n",
    "    np.save(DIR + '/xs.npy', xs)\n",
    "    np.save(DIR + '/ys.npy', ys)\n",
    "else:\n",
    "    xs = np.load(DIR + '/xs.npy')\n",
    "    ys = np.load(DIR + '/ys.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_ph = tf.placeholder(tf.float32, [M, 64, 64, 3])\n",
    "x_ph = tf.placeholder(tf.float32, [M, 64, 64, 3])\n",
    "phase = tf.placeholder(tf.bool)\n",
    "drop = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(samples):\n",
    "  fig = plt.figure(figsize=(4, 4))\n",
    "  plt.title(str(samples))\n",
    "  gs = gridspec.GridSpec(4, 4)\n",
    "  gs.update(wspace=0.05, hspace=0.05)\n",
    "\n",
    "  for i, sample in enumerate(samples):\n",
    "    ax = plt.subplot(gs[i])\n",
    "    plt.axis('off')\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_aspect('equal')\n",
    "    plt.imshow(sample)\n",
    "  return fig\n",
    "\n",
    "\n",
    "def leakyrelu(x, alpha=leak):\n",
    "    return tf.maximum(x, alpha * x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(y, phase, drop):\n",
    "    h = tf.layers.conv2d(y, 32, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d(h, 64, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d(h, 128, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d_transpose(h, 128, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d_transpose(h, 64, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d_transpose(h, 32, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d_transpose(h, 3, 5, padding='same')\n",
    "    return tf.nn.tanh(h)\n",
    "\n",
    "def decoder(x, phase, drop):\n",
    "    h = tf.layers.conv2d(x, 32, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d(h, 64, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d(h, 128, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d_transpose(h, 128, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d_transpose(h, 64, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d_transpose(h, 32, 5, padding='same')\n",
    "    h = slim.batch_norm(h, center=True, scale=True, is_training=phase)\n",
    "    h = tf.nn.relu(h)\n",
    "    h = tf.layers.conv2d_transpose(h, 3, 5, padding='same')\n",
    "    return tf.nn.tanh(h)\n",
    "\n",
    "\n",
    "def discriminative_network(y):\n",
    "    h = tf.layers.conv2d(y, 32, 5, activation=leakyrelu, padding='same')\n",
    "#    h = tf.layers.dropout(h,drop)\n",
    "    h = tf.layers.conv2d(h, 64, 5, activation=leakyrelu, padding='same')\n",
    "#    h = tf.layers.dropout(h,drop)\n",
    "    h = tf.layers.conv2d(h, 64, 5, activation=leakyrelu, padding='same')\n",
    "#    h = tf.layers.dropout(h,drop)\n",
    "    h = tf.layers.conv2d(h, 64, 5, activation=leakyrelu, padding='same')\n",
    "#    h = tf.layers.dropout(h,drop)\n",
    "    h = tf.layers.conv2d(h, 128, 5, activation=leakyrelu, padding='same')\n",
    "#    h = tf.layers.dropout(h,drop)\n",
    "    h = tf.layers.conv2d(h, 128, 5, activation=leakyrelu, padding='same')\n",
    "#    h = tf.layers.dropout(h,drop)\n",
    "    h = tf.layers.conv2d(h, 128, 5, activation=leakyrelu, padding='same')\n",
    "#    h = tf.layers.dropout(h,drop)\n",
    "    h = tf.reshape(h,[M,-1])\n",
    "    logit = slim.fully_connected(h, 1,activation_fn=None)\n",
    "    return logit\n",
    "\n",
    "with tf.variable_scope(\"Gen\"):\n",
    "  yf = decoder(x_ph, phase, drop)\n",
    "  xf = encoder(y_ph, phase, drop)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(0.00001, 0.5, 0.9)\n",
    "optimizer_d = tf.train.AdamOptimizer(0.00001, 0.5, 0.9)\n",
    "#optimizer = tf.train.RMSPropOptimizer(0.00005)\n",
    "#optimizer_d = tf.train.RMSPropOptimizer(0.00005)\n",
    "\n",
    "\n",
    "inference = ed.sharedWGANInference(\n",
    "    data={yf: y_ph, xf: x_ph}, discriminator=discriminative_network)\n",
    "\n",
    "inference.initialize(\n",
    "    optimizer = optimizer, optimizer_d=optimizer_d, n_iter=50000, n_print=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize The Session\n",
    "sess = ed.get_session()\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess.run(init_op)\n",
    "i = 0\n",
    "j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Printing images for iteration: 0\n",
      "  600/50000 [  1%]                                ETA: 70100s | Gen Loss: 108.400 | Disc Loss: 0.000\n",
      " Printing images for iteration: 1\n",
      "  900/50000 [  1%]                                ETA: 69275s | Gen Loss: 109.871 | Disc Loss: 0.000\n",
      " Printing images for iteration: 2\n",
      "\n",
      " Printing images for iteration: 3\n",
      "\n",
      " Printing images for iteration: 4\n",
      "\n",
      " Printing images for iteration: 5\n",
      "\n",
      " Printing images for iteration: 6\n",
      "\n",
      " Printing images for iteration: 7\n",
      "\n",
      " Printing images for iteration: 8\n",
      "\n",
      " Printing images for iteration: 9\n",
      "\n",
      " Printing images for iteration: 10\n",
      "\n",
      " Printing images for iteration: 11\n",
      "\n",
      " Printing images for iteration: 12\n",
      "\n",
      " Printing images for iteration: 13\n",
      "\n",
      " Printing images for iteration: 14\n",
      "\n",
      " Printing images for iteration: 15\n",
      "\n",
      " Printing images for iteration: 16\n",
      "\n",
      " Printing images for iteration: 17\n",
      "\n",
      " Printing images for iteration: 18\n",
      "\n",
      " Printing images for iteration: 19\n",
      "\n",
      " Printing images for iteration: 20\n",
      "\n",
      " Printing images for iteration: 21\n",
      "\n",
      " Printing images for iteration: 22\n",
      "\n",
      " Printing images for iteration: 23\n",
      "\n",
      " Printing images for iteration: 24\n",
      "\n",
      " Printing images for iteration: 25\n",
      "\n",
      " Printing images for iteration: 26\n",
      "\n",
      " Printing images for iteration: 27\n",
      "16800/50000 [ 33%] ██████████                     ETA: 3329s | Gen Loss: 81.628 | Disc Loss: 0.000\n",
      " Printing images for iteration: 28\n",
      "17400/50000 [ 34%] ██████████                     ETA: 4712s | Gen Loss: 91.670 | Disc Loss: 0.000\n",
      " Printing images for iteration: 29\n",
      "18000/50000 [ 36%] ██████████                     ETA: 5947s | Gen Loss: 77.683 | Disc Loss: 0.000\n",
      " Printing images for iteration: 30\n",
      "18600/50000 [ 37%] ███████████                    ETA: 7048s | Gen Loss: 80.166 | Disc Loss: 0.000\n",
      " Printing images for iteration: 31\n",
      "19200/50000 [ 38%] ███████████                    ETA: 8025s | Gen Loss: 78.313 | Disc Loss: 0.000\n",
      " Printing images for iteration: 32\n",
      "19800/50000 [ 39%] ███████████                    ETA: 8898s | Gen Loss: 86.509 | Disc Loss: 0.000\n",
      " Printing images for iteration: 33\n",
      "20400/50000 [ 40%] ████████████                   ETA: 9672s | Gen Loss: 71.240 | Disc Loss: 0.000\n",
      " Printing images for iteration: 34\n",
      "21000/50000 [ 42%] ████████████                   ETA: 10351s | Gen Loss: 86.576 | Disc Loss: 0.000\n",
      " Printing images for iteration: 35\n",
      "21600/50000 [ 43%] ████████████                   ETA: 10946s | Gen Loss: 79.229 | Disc Loss: 0.000\n",
      " Printing images for iteration: 36\n",
      "22200/50000 [ 44%] █████████████                  ETA: 11466s | Gen Loss: 80.148 | Disc Loss: 0.000\n",
      " Printing images for iteration: 37\n",
      "22800/50000 [ 45%] █████████████                  ETA: 11911s | Gen Loss: 78.702 | Disc Loss: 0.000\n",
      " Printing images for iteration: 38\n",
      "23400/50000 [ 46%] ██████████████                 ETA: 12294s | Gen Loss: 74.376 | Disc Loss: 0.000\n",
      " Printing images for iteration: 39\n",
      "24000/50000 [ 48%] ██████████████                 ETA: 12615s | Gen Loss: 71.425 | Disc Loss: 0.000\n",
      " Printing images for iteration: 40\n",
      "24600/50000 [ 49%] ██████████████                 ETA: 12879s | Gen Loss: 83.461 | Disc Loss: 0.000\n",
      " Printing images for iteration: 41\n",
      "25200/50000 [ 50%] ███████████████                ETA: 13094s | Gen Loss: 87.515 | Disc Loss: 0.000\n",
      " Printing images for iteration: 42\n",
      "25800/50000 [ 51%] ███████████████                ETA: 13258s | Gen Loss: 88.049 | Disc Loss: 0.000\n",
      " Printing images for iteration: 43\n",
      "26400/50000 [ 52%] ███████████████                ETA: 13376s | Gen Loss: 77.325 | Disc Loss: 0.000\n",
      " Printing images for iteration: 44\n",
      "27000/50000 [ 54%] ████████████████               ETA: 13454s | Gen Loss: 73.345 | Disc Loss: 0.000\n",
      " Printing images for iteration: 45\n",
      "27600/50000 [ 55%] ████████████████               ETA: 13493s | Gen Loss: 72.285 | Disc Loss: 0.000\n",
      " Printing images for iteration: 46\n",
      "28200/50000 [ 56%] ████████████████               ETA: 13495s | Gen Loss: 70.386 | Disc Loss: 0.000\n",
      " Printing images for iteration: 47\n",
      "28800/50000 [ 57%] █████████████████              ETA: 13460s | Gen Loss: 71.841 | Disc Loss: 0.000\n",
      " Printing images for iteration: 48\n",
      "29400/50000 [ 58%] █████████████████              ETA: 13394s | Gen Loss: 86.985 | Disc Loss: 0.000\n",
      " Printing images for iteration: 49\n",
      "30000/50000 [ 60%] ██████████████████             ETA: 13297s | Gen Loss: 91.224 | Disc Loss: 0.000\n",
      " Printing images for iteration: 50\n",
      "30600/50000 [ 61%] ██████████████████             ETA: 13172s | Gen Loss: 77.329 | Disc Loss: 0.000\n",
      " Printing images for iteration: 51\n",
      "31200/50000 [ 62%] ██████████████████             ETA: 13019s | Gen Loss: 76.339 | Disc Loss: 0.000\n",
      " Printing images for iteration: 52\n",
      "31800/50000 [ 63%] ███████████████████            ETA: 12840s | Gen Loss: 85.170 | Disc Loss: 0.000\n",
      " Printing images for iteration: 53\n",
      "32400/50000 [ 64%] ███████████████████            ETA: 12638s | Gen Loss: 74.618 | Disc Loss: 0.000\n",
      " Printing images for iteration: 54\n",
      "33000/50000 [ 66%] ███████████████████            ETA: 12412s | Gen Loss: 79.496 | Disc Loss: 0.000\n",
      " Printing images for iteration: 55\n",
      "33600/50000 [ 67%] ████████████████████           ETA: 12165s | Gen Loss: 74.397 | Disc Loss: 0.000\n",
      " Printing images for iteration: 56\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-4f2014aeeedf>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m   \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0minference\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0my_ph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_ph\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphase\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"Disc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;31m#     diagn = sess.run(tf.concat([inference.dxt,inference.dxf,inference.dyt,inference.dyf],1),{y_ph: y_batch, x_ph: x_batch, phase: True})\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# #    print(diagn)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lewan\\edward\\edward\\inferences\\shared_wgan_inference.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, feed_dict, variables)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m     \u001b[0minfo_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msharedWGANInference\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[0msess\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\lewan\\edward\\edward\\inferences\\gan_inference.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, feed_dict, variables)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mvariables\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"Disc\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m       _, t, loss_d = sess.run(\n\u001b[1;32m--> 187\u001b[1;33m           [self.train_d, self.increment_t, self.loss_d], feed_dict)\n\u001b[0m\u001b[0;32m    188\u001b[0m       \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lewan\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    893\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 895\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    896\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lewan\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1122\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1124\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1125\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lewan\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1319\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1321\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1322\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1323\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lewan\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1325\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1326\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1327\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1328\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\Lewan\\Anaconda3\\envs\\py35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1306\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1307\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1308\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for t in range(inference.n_iter):\n",
    "  rndint_y = np.random.choice(len(ys),M,replace=False)\n",
    "  rndint_x = np.random.choice(len(xs),M,replace=False)\n",
    "  y_batch = ys[rndint_y,:,:,:]/127.5 - 1.\n",
    "  x_batch = xs[rndint_x,:,:,:]/127.5 - 1.\n",
    "\n",
    "  for _ in range(5):\n",
    "    inference.update(feed_dict={y_ph: y_batch, x_ph: x_batch, phase: True, drop: 0.5}, variables=\"Disc\")\n",
    "#     diagn = sess.run(tf.concat([inference.dxt,inference.dxf,inference.dyt,inference.dyf],1),{y_ph: y_batch, x_ph: x_batch, phase: True})\n",
    "# #    print(diagn)\n",
    "#     asd2 = sess.run([inference.xp,inference.yp],{y_ph: y_batch, x_ph: x_batch, phase: True})\n",
    "# #    print(asd)\n",
    "#     if np.mean(diagn[0]) == 0.0:\n",
    "#         break\n",
    "#     if np.mean(diagn[2]) == 0.0:\n",
    "#         break\n",
    "#   else:\n",
    "#     info_dict = inference.update(feed_dict={y_ph: y_batch, x_ph: x_batch, phase: True}, variables=\"Gen\")\n",
    "#     inference.print_progress(info_dict)\n",
    "#     continue\n",
    "#   break\n",
    "  info_dict = inference.update(feed_dict={y_ph: y_batch, x_ph: x_batch, phase: True, drop: 0.5}, variables=\"Gen\")\n",
    "  inference.print_progress(info_dict)\n",
    "\n",
    "  \n",
    "  if i % inference.n_print == 0:\n",
    "    \n",
    "    idx = np.random.choice(M, 16, replace=False)\n",
    "    \n",
    "    fig = (x_batch[idx,] + 1.)/2.\n",
    "    fig = plot(fig)\n",
    "    plt.savefig(os.path.join(IMG_DIR+'/trainA/', '{}.jpg').format(\n",
    "        str(j).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    fig = (y_batch[idx,] + 1.)/2.\n",
    "    fig = plot(fig)\n",
    "    plt.savefig(os.path.join(IMG_DIR+'/trainB/', '{}.jpg').format(\n",
    "        str(j).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    D1sam = sess.run(xf, feed_dict={y_ph: y_batch, phase: True, drop: 0.0})\n",
    "    fig = (D1sam[idx,] + 1.)/2.\n",
    "    fig = plot(fig)\n",
    "    plt.savefig(os.path.join(IMG_DIR+'/trainBA/', '{}.jpg').format(\n",
    "       str(j).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    D2sam = sess.run(yf, feed_dict={x_ph: x_batch, phase: True, drop: 0.0})\n",
    "    fig = (D2sam[idx,] + 1.)/2.\n",
    "    fig = plot(fig)\n",
    "    plt.savefig(os.path.join(IMG_DIR+'/trainAB/', '{}.jpg').format(\n",
    "        str(j).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    D1recon = sess.run(xf, feed_dict={y_ph:D2sam, phase: True, drop: 0.0})\n",
    "    fig = (D1recon[idx,] + 1.)/2.\n",
    "    fig = plot(fig)\n",
    "    plt.savefig(os.path.join(IMG_DIR+'/trainABA/', '{}.jpg').format(\n",
    "        str(j).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "    \n",
    "    D2recon = sess.run(yf, feed_dict={x_ph: D1sam, phase: True, drop: 0.0})\n",
    "    fig = (D2recon[idx,] + 1.)/2.\n",
    "    fig = plot(fig)\n",
    "    plt.savefig(os.path.join(IMG_DIR+'/trainBAB/', '{}.jpg').format(\n",
    "        str(j).zfill(3)), bbox_inches='tight')\n",
    "    plt.close(fig)\n",
    "              \n",
    "    print('\\n Printing images for iteration: ' + str(j))\n",
    "    j += 1\n",
    "  i += 1\n",
    "\n",
    "inference.finalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_list = tf.get_collection(\n",
    "          tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Disc\")\n",
    "maxs = []\n",
    "mins = []\n",
    "for v in var_list:\n",
    "    maxs = maxs + [sess.run(tf.reduce_max(v))]\n",
    "    mins = mins + [sess.run(tf.reduce_min(v))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numVars = np.prod(tf.zeros([1]).shape)\n",
    "for var in var_list:\n",
    "    numVars += np.prod(var.shape)\n",
    "numVars\n",
    "mins = []\n",
    "for v in var_list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D1sam = sess.run(yf, feed_dict={x_ph: x_batch, phase: True})\n",
    "\n",
    "fig = (D1sam[idx,] + 1.)/2\n",
    "fig = plot(fig)\n",
    "plt.show(fig)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "var_list = tf.get_collection(\n",
    "          tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"Gen\")\n",
    "maxs = []\n",
    "mins = []\n",
    "for v in var_list:\n",
    "    maxs = maxs + [sess.run(tf.reduce_max(v))]\n",
    "    mins = mins + [sess.run(tf.reduce_min(v))]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
